{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Entity Recogniser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Statements & Data Sources\n",
    "\n",
    "The dataset can be in .csv or .txt format. In .txt, the first line should be considered as header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "from IPython.display import clear_output\n",
    "import timeit\n",
    "datasetPath = \"dataset.csv\" # Relative File Path of Dataset w.r.t. script\n",
    "outputPath = \"DomainEntity.csv\" # Relative File Path of Output w.r.t. script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "with open(datasetPath, \"r\") as file:\n",
    "    next(file) # Header\n",
    "    lines = file.readlines()\n",
    "rawData = \" \".join( [ i.strip(\"\\n\").strip().lower() for i in lines ] ) # Raw Data with all the words in lower case, seperated by whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation of Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalLines = len(lines) # Total number of lines\n",
    "limit = int(1.6e-05 * totalLines) + 2 # Limit for frequency of ngrams\n",
    "freqEntity = set() # Set of frequent entities\n",
    "continuityLength = 0 # Number of continuous words which are nouns\n",
    "continuityString = \"\" # String of continuous words which are nouns\n",
    "common = set() # Set of frequent entities which are nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Frequent n-grams\n",
    "\n",
    "Accepts \"n\" and adds the frequent n-grams to set of frequent entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFreqNGram(n, max_n=5):\n",
    "    ngram = Counter(nltk.ngrams(rawData.split(), n))\n",
    "    for gram in ngram.most_common():\n",
    "        if gram[1]>(max_n-n+1)*limit:\n",
    "            freqEntity.add(\" \".join(gram[0])) # Add n-gram to the set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching n-grams\n",
    "\n",
    "Accepts \"n\" and checks whether the n-grams in the sentences are domain entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matchNGram(n):\n",
    "    global continuityLength\n",
    "    global continuityString\n",
    "    ngramSent = nltk.ngrams(continuityString.split(),n)\n",
    "    for gram in ngramSent:\n",
    "        if \" \".join(gram).lower() in freqEntity:\n",
    "            common.add(\" \".join(gram)) # Add entity to the set\n",
    "            # Remove entity from string of continuous nouns\n",
    "            continuityString = continuityString.replace(\" \".join(gram), \"\")\n",
    "            continuityLength -= n\n",
    "            if continuityLength < n:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Domain Entities\n",
    "\n",
    "Extracts the domain entities in a continuous string of nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDomainEntity():\n",
    "    if continuityLength >= 5:\n",
    "        matchNGram(5)\n",
    "    if continuityLength >= 4:\n",
    "        matchNGram(4)\n",
    "    if continuityLength >= 3:\n",
    "        matchNGram(3)\n",
    "    if continuityLength >= 2:\n",
    "        matchNGram(2)\n",
    "    matchNGram(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main\n",
    "\n",
    "Stores frequent n-grams, n ranging from 1 to 5, in a set.\n",
    "Writes to a file the domain entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100 %\n",
      "Run Time: 5.15 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing...\")\n",
    "for n in range(1,6):\n",
    "    getFreqNGram(n)\n",
    "# File Initializiation\n",
    "file = open(outputPath,\"w\")\n",
    "file.write(\"SENTENCE ID,NAMED ENTITES\\n\")\n",
    "start = timeit.default_timer() # For indicating status of loop\n",
    "for sentid, sent in enumerate(lines):\n",
    "    file.write(str(sentid+1) + \",\") # Sentence ID\n",
    "    # Skip sentences of length 1\n",
    "    if len(sent.split()) <= 1:\n",
    "        file.write(\"\\n\")\n",
    "        continue\n",
    "    tagged = nltk.pos_tag(nltk.word_tokenize(sent)) # POS tagged sentence\n",
    "    # Initialisation\n",
    "    continuityLength = 0\n",
    "    continuityString = \"\"\n",
    "    common.clear()\n",
    "    for (word, tag) in tagged:\n",
    "        if tag in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"]:\n",
    "            continuityLength += 1\n",
    "            continuityString = continuityString + word + \" \"\n",
    "        elif continuityLength > 0:\n",
    "            getDomainEntity()\n",
    "            continuityLength = 0\n",
    "            continuityString = \"\"\n",
    "    if continuityLength > 0:\n",
    "         getDomainEntity()\n",
    "    for i in common:\n",
    "        file.write(i + \"|\")\n",
    "    file.write(\"\\n\")\n",
    "    # For indicating status of loop\n",
    "    if sentid % int(totalLines / 1000) == 0:\n",
    "        stop = timeit.default_timer()\n",
    "        if (sentid / totalLines * 100) < 5:\n",
    "            expected_time = \"Calculating...\"\n",
    "        else:\n",
    "            time_perc = timeit.default_timer()\n",
    "            expected_time = str(round(((time_perc - start) / (sentid / totalLines)) / 60, 2)) + \" minutes\"\n",
    "        clear_output(wait = True)\n",
    "        print(\"Progress:\", round(sentid / totalLines * 100, 2), \"%\")\n",
    "        print(\"Run Time:\", round((stop - start) / 60, 2), \"minutes\")\n",
    "        print(\"Expected Run Time:\", expected_time)\n",
    "stop = timeit.default_timer()\n",
    "clear_output(wait = True)\n",
    "print(\"Progress: 100 %\")\n",
    "print(\"Run Time:\", round((stop - start) / 60, 2), \"minutes\")\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python37564bit3e9631d0826c43a5ab54f67732ace22c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
